{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "\n",
    "## *Logistic Regression*\n",
    "\n",
    "\n",
    "- Logistic Regression predicts the probability of an instance belonging\n",
    "to a specific class\n",
    "\n",
    "- Binary Outcome: The dependent variable is binary, indicating the presence or absence of a characteristic or outcome.\n",
    "- Logit Function: The logistic regression model uses the logit function to model the probability of the binary outcome.\n",
    "- Odds and Probability: The model predicts the log-odds of the outcome, which can be converted to a probability.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg)\n",
    "\n",
    "![](https://www.saedsayad.com/images/LogReg_1.png)\n",
    "\n",
    "\n",
    "- Sigmoid Function \n",
    "![](https://cdn.botpenguin.com/assets/website/Sigmoid_Function_90ec70976d.png)\n",
    "\n",
    "- Log Odds\n",
    " Odds ratio is obtained by the probability of an event occurring divided by the probability that it will not occur. and taking the log of Odds ratio will give the log of Odds\n",
    "\n",
    "\n",
    " -When p = sigmoid( f(p) ), then f(p) is the inverse of the sigmoid, which ends up\n",
    "being ‘the logit’ function:\n",
    "\n",
    "- Logit function: the natural logarithm of the odds ratio -> logit(p) = log(p/1-p)\n",
    "- Odds ratio: Ratio of the probability of success to the probability of failure p/1-p\n",
    "-\n",
    "\n",
    "\n",
    "Logistic Regression Optimization:\n",
    "\n",
    "- Cross-Entropy (CE) Loss \n",
    "    Cross-entropy loss, also known as log-loss, is widely used in logistic regression for binary classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. The cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
    "\n",
    "- Maximum Likelihood Estimation (MLE) \n",
    "    Maximum Likelihood Estimation (MLE) is a method used to estimate the parameters of a statistical model. In the context of logistic regression, MLE is used to find the values of the coefficients that maximize the likelihood of the observed data under the model.\n",
    "\n",
    "Solvers\n",
    "- newton-cg (Newton-Conjugate Gradient)\n",
    "- lbfgs (Limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n",
    "- liblinear (Library for Large Linear Classification)\n",
    "- sag (Stochastic Average Gradient)\n",
    "- saga (Stochastic Average Gradient with L1/L2 Regularization)\n",
    "\n",
    "newton-cg: Second-order derivative optimization method that converges faster\n",
    "than first-order methods. Suitable for small to medium-sized datasets\n",
    "\n",
    "lbfgs: Quasi-Newton method that approximates second-order\n",
    "\n",
    "information using a limited amount of memory. Suitable for small to\n",
    "medium-sized datasets\n",
    "\n",
    "liblinear: First-order derivative method that uses a coordinate descent\n",
    "algorithm. Works well for high-dimensional datasets but does not\n",
    "support L1/L2 regularization\n",
    "\n",
    "sag: A first-order derivative method that uses a stochastic gradient\n",
    "descent algorithm with averaging. Suitable for large-scale datasets\n",
    "\n",
    "saga: A variant of 'sag' that supports L1/L2 regularization. Suitable for\n",
    "\n",
    "large-scale datasets and sparse features\n",
    "\n",
    "\n",
    "\n",
    "Choosing the right solver depends on the size of your dataset, the type of regularization you need, and the computational resources available. Here's a guideline:\n",
    "\n",
    "- Small datasets? → newton-cg or lbfgs for faster convergence\n",
    "\n",
    "- High-dimensional datasets? → liblinear may be a good choice\n",
    "\n",
    "- Large-scale datasets? → sag or saga, especially if you have sparse features\n",
    "\n",
    "- Need L1/L2 regularization? → saga for the best performance\n",
    "\n",
    "\n",
    "\n",
    "## Classification Performance\n",
    "\n",
    "- True positive: Model correctly predicts positive for a sample\n",
    "                (model: positive, actual: positive)\n",
    "\n",
    "- False positive: Model falsely predicts positive for a sample\n",
    "                 (model: positive, actual: negative)\n",
    "\n",
    "- True negative: Model correctly predicts negative for a sample\n",
    "            (model: negative, actual: negative)\n",
    "\n",
    "- False negative: Model falsely predicts negative for a sample        \n",
    "(              model: negative, actual: positive)\n",
    "\n",
    "false negative -> very costly (somebody dies)\n",
    "false positive -> has relatively negligible cost (somebody gets shocked but further tests will confirm no disease)\n",
    "\n",
    "\n",
    "## Confusion matrix\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:969/1*d0UCCIF10Soi7VQGxdVrWQ.jpeg)\n",
    "\n",
    "\n",
    "- Precision, on the other hand, asks: Of the relevant cases identified, how many errors were made? (i.e., how precise are my predictions). This is the positive predictive value of the classifier.\n",
    "A high precision indicates that the model is effective at making correct\n",
    "positive predictions and minimizing false positives\n",
    "\n",
    "- Accuracy: The overall correctness of the model.\n",
    "- Recall (Sensitivity or True Positive Rate): The proportion of actual positives that were identified correctly.\n",
    "     How accurately can the algorithm identify all relevant cases? This is the True Positive Rate (TPR) of the classifier, and identifies the hit rate.\n",
    "    Out of all the actual positive cases, how many did the model call out?\n",
    "    Measures the ability of a model to capture all the relevant instances in the    \n",
    "    dataset\n",
    "\n",
    "## Precison Vs Recall\n",
    "- Cancer Screening -- (Higher Recall) Higher recall to ensure that potential cancer cases are not missed\n",
    "- Search Engine Results (Higher Precision) Users expect to receive accurate and relevant results. In this scenario,\n",
    "higher precision is more important\n",
    "- Manufacturing Quality Control (Higher Precision)Higher precision is preferred, as it results in fewer defective products being produced and shipped to customers.\n",
    "\n",
    "\n",
    "- F1 Score\n",
    "The F1 Score (or just F score) is the harmonic mean between Precision & Recall:\n",
    "    - Ranges from 0 to 1\n",
    "    - Balanced way of combining precision and recall into single metric\n",
    "\n",
    "\n",
    "## ROC\n",
    "\n",
    "Definition:\n",
    "ROC AUC represents the area under the ROC curve, which plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various classification thresholds.\n",
    "Interpretation:\n",
    "AUC ranges from 0 to 1\n",
    "AUC of 0.5 indicates random guessing (no discrimination)\n",
    "AUC closer to 1 indicates better model performance\n",
    "AUC of 1 represents perfect classification\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQRhI6JCzC4ZPY1ls847WCcUe5tm0Cf5UYigw&s)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:494/1*EPmzi0GCgdLstsJb6Q8e-w.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
