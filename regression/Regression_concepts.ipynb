{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Regression\n","\n","## Linear Regression \n","\n","Linear regression is a fundamental statistical and machine learning technique used for predictive analysis and modeling relationships between variables.\n","\n","Linear regression aims to predict the value of a dependent variable based on one or more independent variables by fitting a linear equation to the observed data\n","\n","Basic concept:\n","It establishes a linear relationship between variables, represented by a straight line (in simple linear regression) or a hyperplane (in multiple linear regression)\n","\n","Simple linear regression: Involves one independent variable and one dependent variable\n","Multiple linear regression: Involves multiple independent variables and one dependent variable\n","\n","\n","![image](https://images.spiceworks.com/wp-content/uploads/2022/04/07040339/25-4.png)\n","\n","\n","** Assumptions:**\n","Linear regression relies on certain assumptions, such as linearity, independence of errors, homoscedasticity, and normality of residuals.\n","Homoscedasticity: The variance of residuals should be constant across all levels of the independent variables\n","\n","\n","## **UnderFitting OverFitting**\n","\n","![](https://docs.aws.amazon.com/images/machine-learning/latest/dg/images/mlconcepts_image5.png)\n","\n","![](https://cdn.prod.website-files.com/6108e07db6795265f203a636/64491a56b7454104ae269887_Overvvsunder%20%281%29.jpg)\n","\n","\n","Underfitting in machine learning occurs when a model is too simple to capture the underlying patterns in the data. This results in poor performance on both the training set and unseen data.\n","\n","Causes\n","Model Simplicity: Using a model that is too simple, such as a linear model for data that has a non-linear relationship.\n","Insufficient Training: Not training the model long enough or using poorly chosen hyperparameters.\n","Inadequate Data: Having too few training samples or data that does not represent the full range of possible values.\n","High Bias: Models with high bias, such as those that assume overly simplistic relationships, are prone to underfitting\n","\n","Symptoms:\n","Poor performance on both the training set and new data.\n","High bias and low variance in model predictions\n","\n","**Overfitting**\n","Definition:\n","Overfitting occurs when a model becomes too closely adapted to the specific details and noise in the training data, rather than learning the general underlying patterns.\n","\n","Characteristics:\n","High accuracy on training data\n","Poor performance on new, unseen data\n","High variance and low bias in predictions\n","\n","Causes:\n","Complex models that are too flexible relative to the amount of training data\n","Insufficient training data\n","Noisy data with errors or random fluctuations\n","Training for too long or with too many iterations\n","\n","Detection:\n","Large gap between training and validation performance\n","Learning curves that show divergence between training and validation errors\n","Poor performance on cross-validation tests\n","\n","Consequences:\n","Reduced ability to generalize to new data\n","Unreliable predictions in real-world applications\n","Capturing noise rather than true underlying patterns\n","\n","Prevention and mitigation:\n","Use simpler models or reduce model complexity\n","Increase the amount of training data\n","Apply regularization techniques (e.g., L1/L2 regularization)\n","Use early stopping during training\n","Perform cross-validation\n","Use ensemble methods\n","Improve data quality and remove noise\n","\n","**Regularization**\n","How to overcome overfitting: Regularization\n","L1 and L2 regularization: prevent model weights from becoming overly specific,\n","aka prevent big/large weights\n","\n","L1 is good when some features might not be relevant, L1 can drive them to zero\n","L2 is good for correlated features, as it promotes equal features\n","\n","Regularization aims to reduce model complexity and prevent overfitting by adding a penalty term to the loss function.\n","\n","Lasso (L1) Regularization:\n","Encourages sparsity by shrinking some coefficients to exactly zero.\n","Useful for feature selection in high-dimensional datasets.\n","\n","Ridge (L2) Regularization:\n","Shrinks all coefficients towards zero, but rarely makes them exactly zero.\n","Effective in handling multicollinearity.\n","\n","Elastic Net:\n","Combines L1 and L2 penalties, offering a balance between feature selection and handling multicollinearity.\n","\n","\n","![](https://miro.medium.com/v2/resize:fit:1400/1*rVTCIffI2D_-i_CGeHwF6A.png)\n"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
