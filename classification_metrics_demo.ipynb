{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem with Complete Metrics Analysis\n",
    "\n",
    "This notebook demonstrates a complete classification workflow using multiple algorithms and shows all relevant classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_auc_score,\n    roc_curve, precision_recall_curve, average_precision_score,\n    matthews_corrcoef, cohen_kappa_score, log_loss\n)\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set plotting style - fix for newer matplotlib/seaborn versions\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('seaborn')\nsns.set_palette(\"husl\")\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "df['species'] = df['target'].map({0: target_names[0], 1: target_names[1], 2: target_names[2]})\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFeatures:\", feature_names)\n",
    "print(\"\\nTarget Classes:\", target_names)\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Pairplot style visualization\n",
    "for i, feature in enumerate(feature_names):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    \n",
    "    for j, species in enumerate(target_names):\n",
    "        species_data = df[df['species'] == species][feature]\n",
    "        axes[row, col].hist(species_data, alpha=0.7, label=species, bins=15)\n",
    "    \n",
    "    axes[row, col].set_title(f'Distribution of {feature}')\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df[feature_names].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"{target_names[i]}: {count}\")\n",
    "\n",
    "print(\"\\nTest set class distribution:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"{target_names[i]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed\")\n",
    "print(\"Original feature means:\", X_train.mean(axis=0))\n",
    "print(\"Scaled feature means:\", X_train_scaled.mean(axis=0))\n",
    "print(\"Scaled feature std:\", X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multiple classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "trained_models = {}\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    trained_models[name] = clf\n",
    "    \n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all models\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    predictions[name] = model.predict(X_test_scaled)\n",
    "    probabilities[name] = model.predict_proba(X_test_scaled)\n",
    "    \n",
    "print(\"Predictions generated for all models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_metrics(y_true, y_pred, y_proba, model_name):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive classification metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['Precision (Macro)'] = precision_score(y_true, y_pred, average='macro')\n",
    "    metrics['Precision (Micro)'] = precision_score(y_true, y_pred, average='micro')\n",
    "    metrics['Precision (Weighted)'] = precision_score(y_true, y_pred, average='weighted')\n",
    "    metrics['Recall (Macro)'] = recall_score(y_true, y_pred, average='macro')\n",
    "    metrics['Recall (Micro)'] = recall_score(y_true, y_pred, average='micro')\n",
    "    metrics['Recall (Weighted)'] = recall_score(y_true, y_pred, average='weighted')\n",
    "    metrics['F1 Score (Macro)'] = f1_score(y_true, y_pred, average='macro')\n",
    "    metrics['F1 Score (Micro)'] = f1_score(y_true, y_pred, average='micro')\n",
    "    metrics['F1 Score (Weighted)'] = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Advanced metrics\n",
    "    metrics['Matthews Correlation Coefficient'] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics['Cohen\\'s Kappa'] = cohen_kappa_score(y_true, y_pred)\n",
    "    metrics['Log Loss'] = log_loss(y_true, y_proba)\n",
    "    \n",
    "    # Multi-class ROC AUC\n",
    "    try:\n",
    "        metrics['ROC AUC (OvR Macro)'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')\n",
    "        metrics['ROC AUC (OvR Weighted)'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')\n",
    "    except:\n",
    "        metrics['ROC AUC (OvR Macro)'] = 'N/A'\n",
    "        metrics['ROC AUC (OvR Weighted)'] = 'N/A'\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for all models\n",
    "all_metrics = {}\n",
    "for name in trained_models.keys():\n",
    "    all_metrics[name] = calculate_all_metrics(y_test, predictions[name], probabilities[name], name)\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "print(\"Comprehensive Metrics for All Models:\")\n",
    "print(\"=\" * 60)\n",
    "print(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "models = list(all_metrics.keys())\n",
    "accuracies = [all_metrics[model]['Accuracy'] for model in models]\n",
    "axes[0, 0].bar(models, accuracies, color='skyblue')\n",
    "axes[0, 0].set_title('Accuracy Comparison')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# F1 Score comparison\n",
    "f1_scores = [all_metrics[model]['F1 Score (Macro)'] for model in models]\n",
    "axes[0, 1].bar(models, f1_scores, color='lightcoral')\n",
    "axes[0, 1].set_title('F1 Score (Macro) Comparison')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(f1_scores):\n",
    "    axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Precision comparison\n",
    "precisions = [all_metrics[model]['Precision (Macro)'] for model in models]\n",
    "axes[1, 0].bar(models, precisions, color='lightgreen')\n",
    "axes[1, 0].set_title('Precision (Macro) Comparison')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(precisions):\n",
    "    axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Recall comparison\n",
    "recalls = [all_metrics[model]['Recall (Macro)'] for model in models]\n",
    "axes[1, 1].bar(models, recalls, color='gold')\n",
    "axes[1, 1].set_title('Recall (Macro) Comparison')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(recalls):\n",
    "    axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification reports for each model\n",
    "for name in trained_models.keys():\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(classification_report(y_test, predictions[name], target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, model) in enumerate(trained_models.items()):\n",
    "    cm = confusion_matrix(y_test, predictions[name])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, yticklabels=target_names, ax=axes[i])\n",
    "    axes[i].set_title(f'Confusion Matrix - {name}')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "\n",
    "# Remove the last empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ROC Curves and AUC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for multiclass classification (One-vs-Rest)\n",
    "n_classes = len(np.unique(y))\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, model) in enumerate(trained_models.items()):\n",
    "    y_proba = probabilities[name]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        fpr[class_idx], tpr[class_idx], _ = roc_curve(y_test_binarized[:, class_idx], \n",
    "                                                     y_proba[:, class_idx])\n",
    "        roc_auc[class_idx] = roc_auc_score(y_test_binarized[:, class_idx], \n",
    "                                          y_proba[:, class_idx])\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    for class_idx, color in zip(range(n_classes), colors):\n",
    "        axes[i].plot(fpr[class_idx], tpr[class_idx], color=color, lw=2,\n",
    "                    label=f'{target_names[class_idx]} (AUC = {roc_auc[class_idx]:.2f})')\n",
    "    \n",
    "    axes[i].plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    axes[i].set_xlim([0.0, 1.0])\n",
    "    axes[i].set_ylim([0.0, 1.05])\n",
    "    axes[i].set_xlabel('False Positive Rate')\n",
    "    axes[i].set_ylabel('True Positive Rate')\n",
    "    axes[i].set_title(f'ROC Curves - {name}')\n",
    "    axes[i].legend(loc=\"lower right\")\n",
    "\n",
    "# Remove the last empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, model) in enumerate(trained_models.items()):\n",
    "    y_proba = probabilities[name]\n",
    "    \n",
    "    colors = ['blue', 'red', 'green']\n",
    "    for class_idx, color in zip(range(n_classes), colors):\n",
    "        precision, recall, _ = precision_recall_curve(y_test_binarized[:, class_idx], \n",
    "                                                     y_proba[:, class_idx])\n",
    "        avg_precision = average_precision_score(y_test_binarized[:, class_idx], \n",
    "                                              y_proba[:, class_idx])\n",
    "        \n",
    "        axes[i].plot(recall, precision, color=color, lw=2,\n",
    "                    label=f'{target_names[class_idx]} (AP = {avg_precision:.2f})')\n",
    "    \n",
    "    axes[i].set_xlabel('Recall')\n",
    "    axes[i].set_ylabel('Precision')\n",
    "    axes[i].set_title(f'Precision-Recall Curves - {name}')\n",
    "    axes[i].legend(loc=\"lower left\")\n",
    "    axes[i].set_xlim([0.0, 1.0])\n",
    "    axes[i].set_ylim([0.0, 1.05])\n",
    "\n",
    "# Remove the last empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for all models\n",
    "cv_results = {}\n",
    "cv_folds = 5\n",
    "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Performing {cv_folds}-fold cross-validation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in classifiers.items():\n",
    "    # Accuracy scores\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='accuracy')\n",
    "    cv_results[name] = {\n",
    "        'scores': cv_scores,\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  CV Scores: {cv_scores.round(4)}\")\n",
    "    print(f\"  Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Plot cross-validation results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "model_names = list(cv_results.keys())\n",
    "means = [cv_results[name]['mean'] for name in model_names]\n",
    "stds = [cv_results[name]['std'] for name in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "bars = ax.bar(x_pos, means, yerr=stds, capsize=5, color='lightblue', alpha=0.7)\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Cross-Validation Accuracy')\n",
    "ax.set_title('Cross-Validation Results Comparison')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    ax.text(i, mean + std + 0.01, f'{mean:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for models that support it\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_model = trained_models['Random Forest']\n",
    "rf_importance = rf_model.feature_importances_\n",
    "axes[0].bar(feature_names, rf_importance, color='lightgreen')\n",
    "axes[0].set_title('Feature Importance - Random Forest')\n",
    "axes[0].set_ylabel('Importance')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Logistic Regression coefficients (absolute values)\n",
    "lr_model = trained_models['Logistic Regression']\n",
    "lr_coef = np.abs(lr_model.coef_).mean(axis=0)  # Average across classes\n",
    "axes[1].bar(feature_names, lr_coef, color='lightcoral')\n",
    "axes[1].set_title('Feature Coefficients - Logistic Regression')\n",
    "axes[1].set_ylabel('Absolute Coefficient Value')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary\n",
    "summary_data = []\n",
    "\n",
    "for name in trained_models.keys():\n",
    "    summary_data.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': all_metrics[name]['Accuracy'],\n",
    "        'CV Mean': cv_results[name]['mean'],\n",
    "        'CV Std': cv_results[name]['std'],\n",
    "        'F1 Score': all_metrics[name]['F1 Score (Macro)'],\n",
    "        'Precision': all_metrics[name]['Precision (Macro)'],\n",
    "        'Recall': all_metrics[name]['Recall (Macro)'],\n",
    "        'ROC AUC': all_metrics[name]['ROC AUC (OvR Macro)'],\n",
    "        'MCC': all_metrics[name]['Matthews Correlation Coefficient'],\n",
    "        'Cohen\\'s Kappa': all_metrics[name]['Cohen\\'s Kappa']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Model Performance Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.round(4).to_string(index=False))\n",
    "\n",
    "# Find best performing model\n",
    "best_model_idx = summary_df['Test Accuracy'].idxmax()\n",
    "best_model = summary_df.iloc[best_model_idx]['Model']\n",
    "best_accuracy = summary_df.iloc[best_model_idx]['Test Accuracy']\n",
    "\n",
    "print(f\"\\n\\nBest Performing Model: {best_model}\")\n",
    "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some prediction examples with probabilities\n",
    "best_model_obj = trained_models[best_model]\n",
    "sample_indices = [0, 15, 30]  # Show a few examples\n",
    "\n",
    "print(f\"Prediction Examples using {best_model}:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    if idx < len(X_test):\n",
    "        sample_features = X_test_scaled[idx:idx+1]\n",
    "        prediction = best_model_obj.predict(sample_features)[0]\n",
    "        probabilities_sample = best_model_obj.predict_proba(sample_features)[0]\n",
    "        actual = y_test[idx]\n",
    "        \n",
    "        print(f\"\\nSample {idx + 1}:\")\n",
    "        print(f\"  Features: {X_test[idx].round(2)}\")\n",
    "        print(f\"  Actual: {target_names[actual]}\")\n",
    "        print(f\"  Predicted: {target_names[prediction]}\")\n",
    "        print(f\"  Prediction Probabilities:\")\n",
    "        for i, prob in enumerate(probabilities_sample):\n",
    "            print(f\"    {target_names[i]}: {prob:.4f}\")\n",
    "        print(f\"  Correct: {'✓' if prediction == actual else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated a comprehensive classification analysis including:\n",
    "\n",
    "1. **Data Exploration**: Visualized feature distributions and correlations\n",
    "2. **Model Training**: Trained 5 different classification algorithms\n",
    "3. **Comprehensive Metrics**: Calculated accuracy, precision, recall, F1-score, ROC AUC, MCC, Cohen's Kappa, and more\n",
    "4. **Visualization**: Created confusion matrices, ROC curves, and precision-recall curves\n",
    "5. **Cross-Validation**: Performed stratified k-fold cross-validation\n",
    "6. **Feature Importance**: Analyzed which features are most important for classification\n",
    "7. **Model Comparison**: Compared all models across multiple metrics\n",
    "\n",
    "The analysis shows that all models perform exceptionally well on the Iris dataset, which is expected as it's a well-separated, clean dataset. The Random Forest and SVM models show particularly strong performance across all metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}